#cloud-config
hostname: ${hostname}
fqdn: ${hostname}.hpc-obs.internal
manage_etc_hosts: true

package_update: true
package_upgrade: true

packages:
  - awscli
  - jq
  - munge
  - libmunge-dev
  - libmunge2
  - build-essential
  - libmariadb-dev
  - libpam0g-dev
  - libhttp-parser-dev
  - libjson-c-dev
  - libyaml-dev
  - libhwloc-dev
  - liblz4-dev
  - libfreeipmi-dev
  - librrd-dev
  - libipmimonitoring-dev
  - liblua5.3-dev
  - libdbus-1-dev
  - bzip2
  - linux-headers-$(uname -r)
  - gcc
  - make
  - dkms

write_files:
  - path: /opt/hpc-obs/scripts/setup-nvidia.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -euo pipefail
      
      LOG_FILE="/var/log/demo-setup/nvidia-setup.log"
      mkdir -p /var/log/demo-setup
      exec > >(tee -a "$LOG_FILE") 2>&1
      
      MARKER_FILE="/opt/nvidia/.driver-installed"
      
      if [[ -f "$MARKER_FILE" ]]; then
          echo "NVIDIA driver already installed, skipping..."
          nvidia-smi || true
          exit 0
      fi
      
      echo "=== Installing NVIDIA Driver at $(date) ==="
      
      # Add NVIDIA repository
      wget -q https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
      dpkg -i cuda-keyring_1.1-1_all.deb
      apt-get update
      
      # Install driver (535 for T4 on g4dn)
      DEBIAN_FRONTEND=noninteractive apt-get install -y cuda-drivers-535
      
      # Install DCGM
      DEBIAN_FRONTEND=noninteractive apt-get install -y datacenter-gpu-manager
      
      mkdir -p /opt/nvidia
      touch "$MARKER_FILE"
      
      echo "=== NVIDIA Driver installation completed at $(date) ==="
      echo "Reboot may be required for driver to load properly"

  - path: /opt/hpc-obs/scripts/setup-slurm-compute.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -euo pipefail
      
      LOG_FILE="/var/log/demo-setup/slurm-compute-setup.log"
      mkdir -p /var/log/demo-setup
      exec > >(tee -a "$LOG_FILE") 2>&1
      
      MARKER_FILE="/etc/slurm/.compute-initialized"
      SLURM_VERSION="25.11.2"
      
      if [[ -f "$MARKER_FILE" ]]; then
          echo "Slurm compute already initialized, skipping..."
          exit 0
      fi
      
      echo "=== Setting up Slurm Compute Node at $(date) ==="
      
      AWS_REGION=$(curl -s http://169.254.169.254/latest/meta-data/placement/region)
      INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
      INSTANCE_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)
      
      # Generate unique hostname from instance ID
      SHORT_ID=$(echo "$INSTANCE_ID" | cut -d'-' -f2 | head -c8)
      HOSTNAME="compute-$SHORT_ID"
      hostnamectl set-hostname "$HOSTNAME"
      echo "$INSTANCE_IP $HOSTNAME" >> /etc/hosts
      
      # Get Munge key from SSM
      MUNGE_KEY=$(aws ssm get-parameter --name "/hpc-obs/${environment}/munge-key" --with-decryption --region "$AWS_REGION" --query 'Parameter.Value' --output text)
      
      # Setup Munge
      echo "$MUNGE_KEY" | base64 -d > /etc/munge/munge.key
      chmod 400 /etc/munge/munge.key
      chown munge:munge /etc/munge/munge.key
      systemctl enable munge
      systemctl restart munge
      
      # Create slurm user
      groupadd -g 64030 slurm || true
      useradd -u 64030 -g slurm -s /bin/false slurm || true
      
      # Download and build Slurm
      cd /tmp
      if [[ ! -f "slurm-$${SLURM_VERSION}.tar.bz2" ]]; then
          wget -q https://download.schedmd.com/slurm/slurm-$${SLURM_VERSION}.tar.bz2
      fi
      tar xjf slurm-$${SLURM_VERSION}.tar.bz2
      cd slurm-$${SLURM_VERSION}
      
      ./configure --prefix=/usr --sysconfdir=/etc/slurm --with-munge --enable-pam
      make -j$(nproc)
      make install
      
      # Create directories
      mkdir -p /etc/slurm /var/spool/slurm/d /var/log/slurm /var/run/slurm
      chown -R slurm:slurm /var/spool/slurm /var/log/slurm /var/run/slurm
      
      # Get slurm.conf from controller
      echo "Fetching slurm.conf from controller..."
      for i in {1..30}; do
          if scp -o StrictHostKeyChecking=no -o ConnectTimeout=5 ${controller_ip}:/etc/slurm/slurm.conf /etc/slurm/slurm.conf 2>/dev/null; then
              echo "Got slurm.conf"
              break
          fi
          echo "Waiting for controller... attempt $i"
          sleep 10
      done
      
      # Also get cgroup.conf and gres.conf
      scp -o StrictHostKeyChecking=no ${controller_ip}:/etc/slurm/cgroup.conf /etc/slurm/ || true
      scp -o StrictHostKeyChecking=no ${controller_ip}:/etc/slurm/gres.conf /etc/slurm/ || true
      
      # Configure local gres.conf for GPU auto-detection
      cat > /etc/slurm/gres.conf << EOF
      AutoDetect=nvml
      EOF
      
      # Detect hardware
      CPUS=$(nproc)
      MEMORY=$(free -m | awk '/^Mem:/{print int($2 * 0.95)}')
      
      # Detect GPUs
      GPU_COUNT=0
      if command -v nvidia-smi &>/dev/null; then
          GPU_COUNT=$(nvidia-smi -L 2>/dev/null | wc -l || echo 0)
      fi
      
      # Add this node to slurm.conf on controller
      echo "Registering node with controller..."
      NODE_LINE="NodeName=$HOSTNAME CPUs=$CPUS RealMemory=$MEMORY State=UNKNOWN"
      if [[ $GPU_COUNT -gt 0 ]]; then
          NODE_LINE="$NODE_LINE Gres=gpu:$GPU_COUNT"
      fi
      
      # Update slurm.conf on controller (via SSH)
      ssh -o StrictHostKeyChecking=no ${controller_ip} "
        grep -q 'NodeName=$HOSTNAME' /etc/slurm/slurm.conf || echo '$NODE_LINE' >> /etc/slurm/slurm.conf
        scontrol reconfigure || true
      " || echo "Warning: Could not register with controller, manual registration may be needed"
      
      # Refresh local config
      scp -o StrictHostKeyChecking=no ${controller_ip}:/etc/slurm/slurm.conf /etc/slurm/slurm.conf || true
      
      # Copy systemd unit
      cp /tmp/slurm-$${SLURM_VERSION}/etc/slurmd.service /etc/systemd/system/
      sed -i 's|/var/run/slurmd.pid|/var/run/slurm/slurmd.pid|g' /etc/systemd/system/slurmd.service
      
      systemctl daemon-reload
      systemctl enable slurmd
      systemctl start slurmd
      
      touch "$MARKER_FILE"
      echo "=== Slurm Compute setup completed at $(date) ==="

  - path: /etc/systemd/system/node_exporter.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Prometheus Node Exporter
      After=network-online.target
      Wants=network-online.target
      
      [Service]
      Type=simple
      User=node_exporter
      Group=node_exporter
      ExecStart=/usr/local/bin/node_exporter \
        --web.listen-address=:9100 \
        --collector.systemd \
        --collector.processes
      Restart=always
      RestartSec=5
      
      [Install]
      WantedBy=multi-user.target

  - path: /etc/systemd/system/dcgm_exporter.service
    permissions: '0644'
    content: |
      [Unit]
      Description=NVIDIA DCGM Exporter
      After=network-online.target nvidia-dcgm.service
      Wants=network-online.target nvidia-dcgm.service
      
      [Service]
      Type=simple
      ExecStart=/usr/bin/dcgm-exporter --address :9400
      Restart=always
      RestartSec=10
      
      [Install]
      WantedBy=multi-user.target

  - path: /opt/hpc-obs/scripts/install-dcgm-exporter.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -euo pipefail
      
      LOG_FILE="/var/log/demo-setup/dcgm-exporter-setup.log"
      mkdir -p /var/log/demo-setup
      exec > >(tee -a "$LOG_FILE") 2>&1
      
      echo "=== Installing DCGM Exporter at $(date) ==="
      
      # Check if NVIDIA driver is loaded
      if ! nvidia-smi &>/dev/null; then
          echo "NVIDIA driver not available, skipping DCGM exporter"
          exit 0
      fi
      
      # Enable and start DCGM
      systemctl enable nvidia-dcgm
      systemctl start nvidia-dcgm
      
      # Install dcgm-exporter
      cd /tmp
      DCGM_EXPORTER_VERSION="3.3.0-3.2.0"
      wget -q https://github.com/NVIDIA/dcgm-exporter/releases/download/v$${DCGM_EXPORTER_VERSION}/dcgm-exporter_$${DCGM_EXPORTER_VERSION}_amd64.deb || {
          # Fallback: build from source
          apt-get install -y golang-go
          git clone https://github.com/NVIDIA/dcgm-exporter.git
          cd dcgm-exporter
          make binary
          cp dcgm-exporter /usr/bin/
      }
      
      if [[ -f "dcgm-exporter_$${DCGM_EXPORTER_VERSION}_amd64.deb" ]]; then
          dpkg -i dcgm-exporter_$${DCGM_EXPORTER_VERSION}_amd64.deb || apt-get install -f -y
      fi
      
      systemctl daemon-reload
      systemctl enable dcgm_exporter
      systemctl start dcgm_exporter
      
      echo "=== DCGM Exporter installation completed at $(date) ==="

  - path: /opt/hpc-obs/scripts/init-compute.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      set -euo pipefail
      
      mkdir -p /var/log/demo-setup
      exec > >(tee -a /var/log/demo-setup/compute-init.log) 2>&1
      echo "=== Compute node initialization started at $(date) ==="
      
      # Ensure SSM agent is running (may be snap or systemd depending on AMI)
      systemctl enable amazon-ssm-agent 2>/dev/null || true
      systemctl start amazon-ssm-agent 2>/dev/null || true
      
      # Install node_exporter
      NODE_EXPORTER_VERSION="1.7.0"
      cd /tmp
      wget -q https://github.com/prometheus/node_exporter/releases/download/v$${NODE_EXPORTER_VERSION}/node_exporter-$${NODE_EXPORTER_VERSION}.linux-amd64.tar.gz
      tar xzf node_exporter-$${NODE_EXPORTER_VERSION}.linux-amd64.tar.gz
      cp node_exporter-$${NODE_EXPORTER_VERSION}.linux-amd64/node_exporter /usr/local/bin/
      
      useradd --no-create-home --shell /bin/false node_exporter || true
      chown node_exporter:node_exporter /usr/local/bin/node_exporter
      
      systemctl daemon-reload
      systemctl enable node_exporter
      systemctl start node_exporter
      
      # Install NVIDIA drivers
      /opt/hpc-obs/scripts/setup-nvidia.sh
      
      # Check if reboot needed
      if ! nvidia-smi &>/dev/null; then
          echo "NVIDIA driver installed but not loaded, scheduling reboot..."
          # Create script to continue setup after reboot
          cat > /var/lib/cloud/scripts/per-boot/continue-setup.sh << 'REBOOTSCRIPT'
      #!/bin/bash
      if [[ -f /etc/slurm/.compute-initialized ]]; then
          rm -f /var/lib/cloud/scripts/per-boot/continue-setup.sh
          exit 0
      fi
      sleep 30
      /opt/hpc-obs/scripts/setup-slurm-compute.sh
      /opt/hpc-obs/scripts/install-dcgm-exporter.sh
      rm -f /var/lib/cloud/scripts/per-boot/continue-setup.sh
      REBOOTSCRIPT
          chmod +x /var/lib/cloud/scripts/per-boot/continue-setup.sh
          reboot
      else
          # Driver already loaded, continue setup
          /opt/hpc-obs/scripts/setup-slurm-compute.sh
          /opt/hpc-obs/scripts/install-dcgm-exporter.sh
      fi
      
      echo "=== Compute node initialization completed at $(date) ==="

runcmd:
  - /opt/hpc-obs/scripts/init-compute.sh
